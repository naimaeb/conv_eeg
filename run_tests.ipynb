{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcf3254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77f8ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/nelosegui/BIFOLD_work/domain_generalisation/conv_eeg/simpleconv_datasets'\n",
    "model_path = '/home/nelosegui/BIFOLD_work/domain_generalisation/conv_eeg/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718d4cd",
   "metadata": {},
   "source": [
    "## Pairwise wasserstein regularization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5654216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Freq: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaima-elosegui\u001b[0m (\u001b[33mnaima-elosegui-technische-universitat-berlin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nelosegui/BIFOLD_work/domain_generalisation/conv_eeg/wandb/run-20241202_165340-fbaooxrj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/fbaooxrj' target=\"_blank\">fresh-wildflower-14</a></strong> to <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/fbaooxrj' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/fbaooxrj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 160, 4, 70, 5, 22, 4, 250, 9] 3326677 params\n",
      "Split: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nelosegui/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/nelosegui/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49 test: 0.753 \n",
      " 49 test: 0.715 \n",
      " 49 test: 0.715 \n",
      " 49 test: 0.585 \n",
      " 49 test: 0.753  average: 0.705\n",
      "Split: 1\n",
      " 49 test: 0.455 \n",
      " 49 test: 0.365 \n",
      " 49 test: 0.293 \n",
      " 49 test: 0.469 \n",
      " 49 test: 0.474  average: 0.411\n",
      "Split: 2\n",
      " 49 test: 0.778 \n",
      " 49 test: 0.625 \n",
      " 49 test: 0.736 \n",
      " 49 test: 0.689 \n",
      " 49 test: 0.467  average: 0.659\n",
      "Split: 3\n",
      " 49 test: 0.484 \n",
      " 49 test: 0.512 \n",
      " 49 test: 0.519 \n",
      " 23 0.724 0.483 "
     ]
    }
   ],
   "source": [
    "from scripts.scripts import *\n",
    "\n",
    "p          = [0.2,160,4,70,5]\n",
    "dict_config = { \n",
    "'model':'EEGSimpleConv',\n",
    "'params':p,\n",
    "'dataset':'BNCI',\n",
    "'runs':5,\n",
    "'n_epochs':50,\n",
    "'EA':True,\n",
    "'mixup':True,\n",
    "'BN':True,\n",
    "'EOG':False,\n",
    "'Z':'Z0',\n",
    "'path':data_path,\n",
    "'lmso':False,\n",
    "'session':True,\n",
    "'reg_subject':True,\n",
    "'use_wandb':True,\n",
    "'evaluation':'cross',\n",
    "'comment':'baseline',\n",
    "'within':False,\n",
    "'mdl':False,\n",
    "'filter':0.5,\n",
    "'save_model':True,\n",
    "'save_model_path':model_path + '/BNCI_pair_wass_reg',\n",
    "'load_model':False,\n",
    "'preload_reg':False,\n",
    "'online':False\n",
    "}    \n",
    "    \n",
    "    \n",
    "X,Y = load_data(dict_config, wass_reg = True)\n",
    "best_params = dict_config['params']\n",
    "best_score = train_test(best_params,dict_config, X,Y, wass_reg = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c9a2d",
   "metadata": {},
   "source": [
    "## cross subject data example with reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "193d1db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Freq: 250\n"
     ]
    }
   ],
   "source": [
    "from scripts.scripts import *\n",
    "\n",
    "p          = [0.2,160,4,70,5] #0th element corresponds to the temperature parameter for the learning rate\n",
    "dict_config = { \n",
    "'wass_reg':True,\n",
    "'model':'EEGSimpleConv',\n",
    "'params':p,\n",
    "'dataset':'BNCI',\n",
    "'runs':5,\n",
    "'n_epochs':50,\n",
    "'EA':True,\n",
    "'mixup':True,\n",
    "'BN':True,\n",
    "'EOG':False,\n",
    "'Z':'Z0',\n",
    "'path':data_path,\n",
    "'lmso':False,\n",
    "'session':True,\n",
    "'reg_subject':True,\n",
    "'use_wandb':True,\n",
    "'evaluation':'cross',\n",
    "'comment':'baseline',\n",
    "'within':False,\n",
    "'mdl':False,\n",
    "'filter':0.5,\n",
    "'save_model':True,\n",
    "'save_model_path':'/home/nelosegui/BIFOLD_work/domain_generalisation/conv_eeg/results/BNCI_subj_reg',\n",
    "'load_model':False,\n",
    "'preload_reg':False,\n",
    "'online':False\n",
    "}    \n",
    "    \n",
    "    \n",
    "X,Y = load_data(dict_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e431d61",
   "metadata": {},
   "source": [
    "## Data explanation for BNCI\n",
    "\n",
    "Batch data is organized in a list of (torch.tensor(data), torch.tensor(class_label), torch.tensor(subject_label))\n",
    "- torch.tensor(data) = (n_trails per session, n_channels, n_timepoints (Hz * seconds))\n",
    "- torch.tensor (class_label): (n_traisls)\n",
    "    - all subjects have the same order of class labels except:\n",
    "    - subj 8,5,0 (session 0) and 3 (session 1) are different from the rest but the same among eachother\n",
    "    - subj 3 session 0 is different from all the rest\n",
    "\n",
    "- torch.tensor(subject): (n_trials) consecutive batches are from the same subject (i.e. 0 and 1, 2 and 3, etc.). Each batch per subject corresponds to a different session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528bf12",
   "metadata": {},
   "source": [
    "## Within data example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc346afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nelosegui/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = loaders(3,X,Y,dict_config['lmso'],5,dict_config['session'],dict_config['reg_subject'], wass_reg = True, within=dict_config['within'],mdl=dict_config['mdl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9b31b",
   "metadata": {},
   "source": [
    "## Funciton that loads pairs of batches of data, aligns thieir classes and computes a distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f36f8896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx 0\n",
      "batch_subject tensor([0])\n",
      "batch_idx 1\n",
      "batch_subject tensor([0])\n",
      "batch_idx 2\n",
      "batch_subject tensor([1])\n",
      "batch_idx 3\n",
      "batch_subject tensor([1])\n",
      "batch_idx 4\n",
      "batch_subject tensor([2])\n",
      "batch_idx 5\n",
      "batch_subject tensor([2])\n",
      "batch_idx 6\n",
      "batch_subject tensor([4])\n",
      "batch_idx 7\n",
      "batch_subject tensor([4])\n",
      "batch_idx 8\n",
      "batch_subject tensor([5])\n",
      "batch_idx 9\n",
      "batch_subject tensor([5])\n",
      "batch_idx 10\n",
      "batch_subject tensor([6])\n",
      "batch_idx 11\n",
      "batch_subject tensor([6])\n",
      "batch_idx 12\n",
      "batch_subject tensor([7])\n",
      "batch_idx 13\n",
      "batch_subject tensor([7])\n",
      "batch_idx 14\n",
      "batch_subject tensor([8])\n",
      "batch_idx 15\n",
      "batch_subject tensor([8])\n"
     ]
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)\n",
    "\n",
    "for batch_idx, batch_data in enumerate(train_loader):\n",
    "    print('batch_idx', batch_idx)\n",
    "    print('batch_subject', torch.unique(batch_data[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2b13e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing batch 0 and batch 2\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([0]) tensor([1])\n",
      "---\n",
      "Comparing batch 1 and batch 3\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([0]) tensor([1])\n",
      "---\n",
      "Comparing batch 2 and batch 4\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([1]) tensor([2])\n",
      "---\n",
      "Comparing batch 3 and batch 5\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([1]) tensor([2])\n",
      "---\n",
      "Comparing batch 4 and batch 6\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([2]) tensor([4])\n",
      "---\n",
      "Comparing batch 5 and batch 7\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([2]) tensor([4])\n",
      "---\n",
      "Comparing batch 6 and batch 8\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([4]) tensor([5])\n",
      "---\n",
      "Comparing batch 7 and batch 9\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([4]) tensor([5])\n",
      "---\n",
      "Comparing batch 8 and batch 10\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([5]) tensor([6])\n",
      "---\n",
      "Comparing batch 9 and batch 11\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([5]) tensor([6])\n",
      "---\n",
      "Comparing batch 10 and batch 12\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([6]) tensor([7])\n",
      "---\n",
      "Comparing batch 11 and batch 13\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([6]) tensor([7])\n",
      "---\n",
      "Comparing batch 12 and batch 14\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([7]) tensor([8])\n",
      "---\n",
      "Comparing batch 13 and batch 15\n",
      "Difference in classes: tensor(0)\n",
      "Subject labels: tensor([7]) tensor([8])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def align_classes(batch_a, batch_b):\n",
    "        '''\n",
    "        Aligns batch class labels to be able to compare domains of classes\n",
    "        batch_a/batch_b: list of torch.tensors (data, class_labels, subject_labels)\n",
    "\n",
    "        Returns:\n",
    "        batch_a_aligned, batch_b_aligned: list of torch.tensors (data, class_labels, subject_labels) where class_labels of a and b are the same.\n",
    "        '''\n",
    "\n",
    "        sorted_a = torch.argsort(batch_a[1])\n",
    "        sorted_b = torch.argsort(batch_b[1])\n",
    "        batch_a_aligned = [batch_a[0][sorted_a], batch_a[1][sorted_a], batch_a[2][sorted_a]]\n",
    "        batch_b_aligned = [batch_b[0][sorted_b], batch_b[1][sorted_b], batch_b[2][sorted_b]]\n",
    "\n",
    "        perm = torch.randperm(len(batch_a[1]))\n",
    "        batch_a_perm= [batch_a_aligned[0][perm], batch_a_aligned[1][perm], batch_a_aligned[2][perm]]\n",
    "        batch_b_perm= [batch_b_aligned[0][perm], batch_b_aligned[1][perm], batch_b_aligned[2][perm]]\n",
    "\n",
    "        return batch_a_perm, batch_b_perm\n",
    "\n",
    "\n",
    "train_loader_iter = iter(train_loader)\n",
    "batches = list(train_loader_iter)\n",
    "\n",
    "for i in range(len(batches) - 2):\n",
    "    batch_0 = batches[i]\n",
    "    batch_2 = batches[i + 2]\n",
    "\n",
    "    batch_0_aligned, batch_2_aligned = align_classes(batch_0, batch_2)\n",
    "\n",
    "    print(f'Comparing batch {i} and batch {i + 2}')\n",
    "    print('Difference in classes:', torch.sum(batch_0_aligned[1] != batch_2_aligned[1]))\n",
    "    print('Subject labels:', torch.unique(batch_0[2]), torch.unique(batch_2[2]))\n",
    "    print('---')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c115f8",
   "metadata": {},
   "source": [
    "## Test wasserstein distance loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae456a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.EEGSimpleConv import EEGSimpleConv\n",
    "import numpy as np\n",
    "import random\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "import wandb \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = EEGSimpleConv(160,4,70,5,22,4,250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c657cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : Cuda libraries were not detected on the system or could not be loaded ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from geomloss import SamplesLoss  # See also ImagesLoss, VolumesLoss\n",
    "\n",
    "# Create some large point clouds in 3D\n",
    "x = torch.randn(100, 3, requires_grad=True).cuda()\n",
    "y = torch.randn(200, 3).cuda()\n",
    "\n",
    "# Define a Sinkhorn (~Wasserstein) loss between sampled measures\n",
    "loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "\n",
    "L = loss(x, y)  # By default, use constant weights = 1/number of samples\n",
    "g_x, = torch.autograd.grad(L, [x])  # GeomLoss fully supports autograd!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74d87cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing batches\n",
      "batch_idx 0\n",
      "batch_curr tensor([3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0,\n",
      "        1, 3, 2, 3, 3, 0, 3, 3, 0, 2, 3, 0, 1, 2, 2, 2, 0, 2, 3, 0, 3, 2, 0, 1,\n",
      "        3, 1, 0, 3, 1, 0, 1, 2, 2, 1, 3, 1, 3, 0, 0, 1, 0, 2, 2, 1, 0, 1, 2, 1])\n",
      "Comparing batches\n",
      "batch_idx 1\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 2\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 3\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 4\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 5\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 6\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 7\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 8\n",
      "batch_curr tensor([3, 2, 2, 1, 3, 3, 0, 2, 0, 1, 1, 2, 3, 3, 2, 3, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 2, 2, 1, 0, 2, 3, 3, 1, 3, 1, 3, 2, 0, 3, 2, 0, 2, 0, 1, 2, 3, 1,\n",
      "        0, 1, 1, 3, 1, 3, 3, 2, 3, 2, 3, 0, 1, 2, 0, 1, 1, 3, 3, 2, 1, 0, 3, 3,\n",
      "        3, 0, 0, 0, 1, 2, 2, 2, 0, 0, 2, 0, 1, 0, 0, 2, 2, 1, 2, 1, 1, 3, 3, 2,\n",
      "        2, 3, 1, 3, 2, 3, 1, 2, 3, 3, 2, 0, 2, 1, 1, 2, 3, 1, 3, 2, 1, 0, 0, 1,\n",
      "        2, 3, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 1, 3, 1, 2, 0, 2, 1, 2, 2, 3, 0, 3,\n",
      "        1, 1, 2, 3, 1, 3, 3, 3, 1, 0, 3, 3, 2, 2, 3, 1, 3, 0, 2, 0, 2, 3, 1, 1,\n",
      "        2, 0, 1, 3, 1, 0, 0, 0, 2, 2, 2, 0, 1, 2, 1, 3, 3, 0, 0, 2, 2, 1, 0, 0,\n",
      "        3, 1, 0, 3, 2, 2, 0, 0, 1, 1, 2, 2, 1, 3, 1, 2, 0, 1, 3, 2, 2, 2, 1, 2,\n",
      "        2, 1, 3, 3, 3, 1, 1, 0, 3, 3, 0, 2, 0, 3, 3, 1, 0, 0, 0, 0, 1, 0, 2, 3,\n",
      "        1, 1, 0, 0, 1, 0, 0, 3, 2, 2, 3, 2, 0, 3, 0, 2, 1, 2, 3, 1, 0, 2, 0, 3,\n",
      "        1, 1, 2, 3, 1, 3, 2, 2, 0, 0, 3, 3, 0, 1, 1, 2, 1, 3, 2, 3, 3, 1, 2, 0])\n",
      "Comparing batches\n",
      "batch_idx 9\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 10\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 11\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 12\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 13\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n",
      "Comparing batches\n",
      "batch_idx 14\n",
      "batch_curr tensor([3, 2, 2, 1, 3, 3, 0, 2, 0, 1, 1, 2, 3, 3, 2, 3, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 2, 2, 1, 0, 2, 3, 3, 1, 3, 1, 3, 2, 0, 3, 2, 0, 2, 0, 1, 2, 3, 1,\n",
      "        0, 1, 1, 3, 1, 3, 3, 2, 3, 2, 3, 0, 1, 2, 0, 1, 1, 3, 3, 2, 1, 0, 3, 3,\n",
      "        3, 0, 0, 0, 1, 2, 2, 2, 0, 0, 2, 0, 1, 0, 0, 2, 2, 1, 2, 1, 1, 3, 3, 2,\n",
      "        2, 3, 1, 3, 2, 3, 1, 2, 3, 3, 2, 0, 2, 1, 1, 2, 3, 1, 3, 2, 1, 0, 0, 1,\n",
      "        2, 3, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 1, 3, 1, 2, 0, 2, 1, 2, 2, 3, 0, 3,\n",
      "        1, 1, 2, 3, 1, 3, 3, 3, 1, 0, 3, 3, 2, 2, 3, 1, 3, 0, 2, 0, 2, 3, 1, 1,\n",
      "        2, 0, 1, 3, 1, 0, 0, 0, 2, 2, 2, 0, 1, 2, 1, 3, 3, 0, 0, 2, 2, 1, 0, 0,\n",
      "        3, 1, 0, 3, 2, 2, 0, 0, 1, 1, 2, 2, 1, 3, 1, 2, 0, 1, 3, 2, 2, 2, 1, 2,\n",
      "        2, 1, 3, 3, 3, 1, 1, 0, 3, 3, 0, 2, 0, 3, 3, 1, 0, 0, 0, 0, 1, 0, 2, 3,\n",
      "        1, 1, 0, 0, 1, 0, 0, 3, 2, 2, 3, 2, 0, 3, 0, 2, 1, 2, 3, 1, 0, 2, 0, 3,\n",
      "        1, 1, 2, 3, 1, 3, 2, 2, 0, 0, 3, 3, 0, 1, 1, 2, 1, 3, 2, 3, 3, 1, 2, 0])\n",
      "Comparing batches\n",
      "batch_idx 15\n",
      "batch_curr tensor([1, 2, 2, 1, 2, 1, 2, 0, 2, 3, 1, 0, 2, 1, 3, 3, 3, 3, 3, 1, 0, 2, 1, 1,\n",
      "        0, 3, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 3, 0, 0, 3, 3, 3, 3,\n",
      "        3, 0, 2, 1, 1, 2, 0, 3, 2, 0, 1, 1, 1, 3, 2, 2, 1, 1, 0, 1, 2, 3, 3, 0,\n",
      "        1, 3, 3, 2, 3, 3, 2, 1, 2, 0, 0, 0, 3, 0, 1, 3, 2, 0, 2, 0, 3, 2, 0, 1,\n",
      "        1, 1, 3, 2, 1, 0, 1, 0, 2, 3, 1, 0, 0, 1, 0, 2, 3, 3, 3, 0, 1, 3, 2, 3,\n",
      "        2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 3, 2, 3, 0, 2, 2, 2, 0, 3, 1,\n",
      "        2, 3, 1, 0, 0, 3, 1, 1, 0, 2, 3, 3, 3, 2, 1, 0, 2, 3, 1, 3, 0, 2, 3, 3,\n",
      "        1, 2, 2, 0, 3, 2, 1, 1, 3, 2, 1, 0, 2, 2, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1,\n",
      "        2, 1, 1, 0, 0, 2, 0, 3, 1, 3, 1, 1, 2, 3, 0, 2, 3, 0, 3, 0, 3, 2, 2, 3,\n",
      "        1, 2, 2, 2, 0, 3, 1, 3, 1, 0, 1, 3, 1, 0, 1, 2, 0, 0, 3, 1, 2, 3, 2, 0,\n",
      "        0, 1, 3, 2, 3, 1, 1, 0, 0, 2, 3, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 3, 3, 0,\n",
      "        3, 1, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 1, 0, 0, 3, 2, 0, 0, 0, 3, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "\n",
    "for batch_idx, batch_curr in enumerate(data_iter):\n",
    "    \n",
    "    # Compare batch1 to batch3 and batch2 to batch4\n",
    "    print('Comparing batches')\n",
    "    print('batch_idx', batch_idx)\n",
    "    print('batch_curr', batch_curr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4194271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = batch_data[0][0][0]\n",
    "input.shape[0]\n",
    "#batch_norm = torch.nn.BatchNorm1d(3)\n",
    "#input_normalized = batch_norm(input)\n",
    "#input_normalized,input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b47120b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([10, 5])\n",
      "output tensor([[-0.8610,  0.7582,  1.2295, -0.7259, -0.2914],\n",
      "        [ 0.6025,  0.5830, -1.1238, -0.4869, -0.8016],\n",
      "        [-1.2765,  1.5009, -0.7454, -0.2863, -0.6954],\n",
      "        [-0.4101, -2.1724, -0.2423,  2.3968, -1.9922],\n",
      "        [ 1.5743, -1.0058,  0.7302,  0.6021,  0.8089],\n",
      "        [ 1.4122, -0.4300, -0.1817,  0.7708,  1.5848],\n",
      "        [-1.2578,  0.3015, -0.9362, -1.0740, -0.3567],\n",
      "        [-0.2358, -0.5539, -0.2136,  0.2725,  0.5640],\n",
      "        [ 0.8914,  0.5185, -0.6765, -0.4650,  1.0964],\n",
      "        [-0.4392,  0.5000,  2.1599, -1.0042,  0.0834]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "input tensor([[-1.3044,  0.2933,  0.8366, -0.7400, -0.2320],\n",
      "        [ 0.4637,  0.1587, -0.9045, -0.5656, -0.7003],\n",
      "        [-1.8064,  0.8643, -0.6245, -0.4193, -0.6029],\n",
      "        [-0.7597, -1.9598, -0.2523,  1.5386, -1.7933],\n",
      "        [ 1.6379, -1.0629,  0.4672,  0.2290,  0.7781],\n",
      "        [ 1.4420, -0.6202, -0.2075,  0.3521,  1.4905],\n",
      "        [-1.7839, -0.0578, -0.7657, -0.9940, -0.2919],\n",
      "        [-0.5491, -0.7155, -0.2311, -0.0115,  0.5533],\n",
      "        [ 0.8128,  0.1091, -0.5736, -0.5496,  1.0421],\n",
      "        [-0.7949,  0.0948,  1.5249, -0.9431,  0.1121]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom layer with BatchNorm\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim)  # Apply BatchNorm to output features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)    # Apply BatchNorm\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_data = torch.randn(10, 5)  # Batch of 10 samples, each with 5 features\n",
    "layer = CustomLayer(input_dim=5, output_dim=5)\n",
    "\n",
    "output = layer(input_data)\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print('output', output)\n",
    "print('input', input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c967de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data: tensor([[-0.7937, -0.7865,  1.0887],\n",
      "        [-0.2109,  0.4925, -0.2563],\n",
      "        [ 0.8570,  1.0815, -0.4006],\n",
      "        [-0.5012, -0.3917,  1.7267],\n",
      "        [ 1.5940,  0.1041, -0.6782],\n",
      "        [ 1.1589,  0.5656,  0.8356],\n",
      "        [-0.2532,  1.1831,  0.7552],\n",
      "        [ 1.5578,  0.0066, -1.0327],\n",
      "        [-0.6860, -0.6093, -0.5870],\n",
      "        [-0.6943, -0.3133, -1.4610]])\n",
      "Batch labels: tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1])\n",
      "Batch data: tensor([[-2.1095, -0.4019, -1.7926],\n",
      "        [-0.6349, -0.0182,  0.7703],\n",
      "        [-1.0121, -0.8909, -0.2404],\n",
      "        [-0.1838, -1.1900,  0.2608],\n",
      "        [ 0.3238,  0.8002, -0.4169],\n",
      "        [ 0.9614,  0.9133,  0.9691],\n",
      "        [ 0.6174,  0.0676, -2.0867],\n",
      "        [ 1.6331,  0.0657,  1.0215],\n",
      "        [ 1.0043, -0.3438, -0.1344],\n",
      "        [ 0.3274, -1.2679, -0.6764]])\n",
      "Batch labels: tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "Batch data: tensor([[-8.4147e-01,  1.7601e-03,  1.3723e+00],\n",
      "        [-7.8138e-01,  1.2383e+00, -1.0685e+00],\n",
      "        [ 7.8584e-01,  2.7759e+00, -1.6949e-01],\n",
      "        [ 9.6302e-01, -2.0497e+00,  5.1462e-01],\n",
      "        [ 4.1230e-01, -1.9535e-01, -5.7200e-01],\n",
      "        [-8.1467e-02,  7.2580e-01,  1.1775e+00],\n",
      "        [-2.9847e-01,  1.4860e-01, -8.9514e-01],\n",
      "        [ 6.1320e-01, -6.0846e-01, -5.0476e-01],\n",
      "        [ 9.5415e-01,  1.4027e+00,  1.2237e-01],\n",
      "        [-6.9577e-01,  1.5268e-01, -3.5552e-01]])\n",
      "Batch labels: tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0])\n",
      "Batch data: tensor([[ 1.2302, -0.8498,  0.5964],\n",
      "        [-0.6266,  0.0688,  1.4240],\n",
      "        [ 0.4381,  0.2284,  0.5329],\n",
      "        [-0.6626, -0.9343, -0.2163],\n",
      "        [ 0.2009, -1.2310, -1.2539],\n",
      "        [-0.5697, -1.1535, -2.0803],\n",
      "        [-1.7762,  1.5462,  0.3202],\n",
      "        [-0.5798,  0.5929, -0.2831],\n",
      "        [-0.0046,  1.9295, -1.2739],\n",
      "        [-0.5987, -0.9041,  1.5978]])\n",
      "Batch labels: tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0])\n",
      "Batch data: tensor([[ 2.1648,  0.1861, -1.1539],\n",
      "        [ 0.2229,  2.0897,  0.6439],\n",
      "        [ 1.2946, -1.1594,  1.6180],\n",
      "        [ 2.5129,  0.0720,  1.0178],\n",
      "        [-0.0878,  0.2920, -1.6048],\n",
      "        [ 0.1335,  0.2917,  0.7323],\n",
      "        [ 1.1784, -0.0456,  1.0505],\n",
      "        [-0.5708,  0.5370, -0.1735],\n",
      "        [-0.4440,  1.1517,  0.4114],\n",
      "        [-1.2065,  1.2324,  0.9966]])\n",
      "Batch labels: tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1])\n",
      "Batch data: tensor([[-1.9704,  1.6075,  0.3711],\n",
      "        [-0.0800,  1.0876,  0.0613],\n",
      "        [-0.6702,  0.9196,  1.9217],\n",
      "        [-1.0040, -0.7319, -1.3120],\n",
      "        [ 0.9775, -1.2594, -0.0529],\n",
      "        [ 0.5788, -0.6518, -0.2581],\n",
      "        [ 0.9911,  0.0326,  1.0245],\n",
      "        [ 0.3861,  2.3768, -0.8442],\n",
      "        [-0.0982,  0.1216, -0.0630],\n",
      "        [-0.5660,  0.4357,  1.2414]])\n",
      "Batch labels: tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1])\n",
      "Batch data: tensor([[-0.7714,  2.1331, -1.5073],\n",
      "        [-0.6121,  0.5736, -0.7677],\n",
      "        [ 1.1545, -0.0487, -0.0566],\n",
      "        [ 0.6544,  1.4806,  0.6337],\n",
      "        [-1.5315, -0.4803,  0.1165],\n",
      "        [ 1.4610, -0.8362, -0.8016],\n",
      "        [-0.7155,  1.1492, -0.9785],\n",
      "        [-0.0096,  0.5836, -1.6492],\n",
      "        [ 2.4363,  0.3645,  0.8004],\n",
      "        [ 1.4674,  0.6088,  0.3152]])\n",
      "Batch labels: tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1])\n",
      "Batch data: tensor([[-1.3414,  0.3766, -0.6547],\n",
      "        [-0.9755, -0.4524,  0.5999],\n",
      "        [ 0.3223,  0.9501, -0.2991],\n",
      "        [ 2.0702,  0.7192,  1.1120],\n",
      "        [ 0.8451, -0.5797,  1.1188],\n",
      "        [ 0.9875, -1.0898,  0.1461],\n",
      "        [-0.9785, -1.1007,  0.2391],\n",
      "        [-0.2828,  0.0564,  0.5505],\n",
      "        [-0.7434,  2.0906, -0.4800],\n",
      "        [-1.3443, -0.5181,  0.8122]])\n",
      "Batch labels: tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1])\n",
      "Batch data: tensor([[ 1.7491,  1.5594,  0.8689],\n",
      "        [ 0.1706, -0.4735,  0.3980],\n",
      "        [ 1.3193,  0.7483,  0.5533],\n",
      "        [-0.1243,  0.9452, -0.1089],\n",
      "        [-0.0883,  0.1191, -1.0406],\n",
      "        [-0.3020,  0.2144, -1.8872],\n",
      "        [ 0.3397,  0.5366, -0.6415],\n",
      "        [ 0.1016, -1.8387, -0.6889],\n",
      "        [-0.4588,  0.6144,  0.1558],\n",
      "        [ 0.1361, -0.5922,  1.1220]])\n",
      "Batch labels: tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0])\n",
      "Batch data: tensor([[-1.0683, -0.5613,  0.0181],\n",
      "        [-0.2487,  0.4299,  1.0282],\n",
      "        [-3.2632,  0.7151, -0.0289],\n",
      "        [-0.2793, -0.4013, -0.6611],\n",
      "        [ 0.0981,  0.1564,  0.4120],\n",
      "        [-0.2646, -0.0977,  0.6388],\n",
      "        [ 1.0626, -1.0615, -0.0784],\n",
      "        [-1.1613,  0.7800, -0.3806],\n",
      "        [-0.2352,  0.2587,  1.2739],\n",
      "        [-0.1972,  1.7915,  0.3888]])\n",
      "Batch labels: tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example dataset\n",
    "data = torch.randn(100, 3)  # 100 samples, 3 features each\n",
    "labels = torch.randint(0, 2, (100,))  # 100 labels (binary classification)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(data, labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch_data, batch_labels in dataloader:\n",
    "    print(\"Batch data:\", batch_data)\n",
    "    print(\"Batch labels:\", batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04aeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = dict_config['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206d683-1459-4603-83a2-6064ea90b6e8",
   "metadata": {},
   "source": [
    "# Cross-Subject on BNCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "991d0b4e-f8dc-4890-92a8-4dbcf44e300d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Freq: 250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zg4ah5xj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-forest-6</strong> at: <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/zg4ah5xj' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/zg4ah5xj</a><br/> View project at: <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_175648-zg4ah5xj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zg4ah5xj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nelosegui/BIFOLD_work/domain_generalisation/conv_eeg/wandb/run-20241130_180239-bxt8pbc8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/bxt8pbc8' target=\"_blank\">stilted-glitter-7</a></strong> to <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/bxt8pbc8' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/bxt8pbc8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109, 4, 70, 8, 22, 4, 250] 2462246 params\n",
      "Split: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nelosegui/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50 0.598 0.700 "
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'latent' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m X,Y \u001b[38;5;241m=\u001b[39m load_data(dict_config)\n\u001b[1;32m     35\u001b[0m best_params \u001b[38;5;241m=\u001b[39m dict_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 36\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdict_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BIFOLD_work/domain_generalisation/conv_eeg/scripts/scripts.py:355\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(params, dict_config, X, Y)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dict_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_model\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    354\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m:model\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()},dict_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_model_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/model_w_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(idx_)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(n_run)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 355\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdict_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpreload_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpreload_reg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    357\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m~/BIFOLD_work/domain_generalisation/conv_eeg/scripts/scripts.py:285\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(epoch, model, test_loader, bn, confusions, preload_reg)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m : \n\u001b[1;32m    283\u001b[0m     (data, target) \u001b[38;5;241m=\u001b[39m batch_data\n\u001b[0;32m--> 285\u001b[0m data, target, latent \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device), \u001b[43mlatent\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg_subject \u001b[38;5;129;01mor\u001b[39;00m preload_reg:\n\u001b[1;32m    288\u001b[0m     output,_ \u001b[38;5;241m=\u001b[39m model(data)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'latent' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from scripts.scripts import *\n",
    "\n",
    "\n",
    "p          = [109,4,70,8] \n",
    "dict_config = { \n",
    "'model':'EEGSimpleConv',\n",
    "'params':p,\n",
    "'dataset':'BNCI',\n",
    "'runs':5,\n",
    "'n_epochs':50,\n",
    "'EA':True,\n",
    "'mixup':True,\n",
    "'BN':True,\n",
    "'EOG':False,\n",
    "'Z':'Z0',\n",
    "'path': data_path,\n",
    "'lmso':False,\n",
    "'session':True,\n",
    "'reg_subject':False,\n",
    "'use_wandb':True,\n",
    "'evaluation':'cross',\n",
    "'comment':'baseline',\n",
    "'within':False,\n",
    "'mdl':False,\n",
    "'filter':0.5,\n",
    "'save_model':True,\n",
    "'save_model_path': model_path + '/BNCI_cross_subj',\n",
    "'load_model':False,\n",
    "'preload_reg':False,\n",
    "'online':False\n",
    "}\n",
    "    \n",
    "    \n",
    "X,Y = load_data(dict_config)\n",
    "best_params = dict_config['params']\n",
    "best_score = train_test(best_params,dict_config, X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4db68a-02c9-4453-a6e3-875ba555ddb3",
   "metadata": {},
   "source": [
    "# Cross-Subject on BNCI with subject-wise reguralization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c50e2ee-6fc5-4695-afdb-2dc76754a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of scripts.scripts failed: Traceback (most recent call last):\n",
      "  File \"/home/nelosegui/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/nelosegui/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/nelosegui/BIFOLD_work/domain_generalisation/conv_eeg/scripts/scripts.py\", line 197\n",
      "    def train(epoch, model, criterion, optimizer, train_loader, mixup = False,T=0.1,preload_reg=False):\n",
      "    ^^^\n",
      "IndentationError: expected an indented block after function definition on line 195\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Freq: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaima-elosegui\u001b[0m (\u001b[33mnaima-elosegui-technische-universitat-berlin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nelosegui/BIFOLD_work/domain_generalisation/conv_eeg/wandb/run-20241130_175648-zg4ah5xj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/zg4ah5xj' target=\"_blank\">happy-forest-6</a></strong> to <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/zg4ah5xj' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/simpleconv_c/runs/zg4ah5xj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 160, 4, 70, 5, 22, 4, 250, 9] 3326677 params\n",
      "Split: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nelosegui/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/nelosegui/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50 0.452 0.652 "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m X,Y \u001b[38;5;241m=\u001b[39m load_data(dict_config)\n\u001b[1;32m     34\u001b[0m best_params \u001b[38;5;241m=\u001b[39m dict_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 35\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdict_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BIFOLD_work/domain_generalisation/conv_eeg/scripts/scripts.py:352\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(params, dict_config, X, Y)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m for epoch in range(dict_config['n_epochs']):\n\u001b[0;32m--> 352\u001b[0m     train_acc = train(epoch, model, criterion, optimizer, train_loader, mixup = dict_config['mixup'],T=dict_config['T'],preload_reg=dict_config['preload_reg'])\n\u001b[1;32m    353\u001b[0m     if epoch ==dict_config['n_epochs'] -1 : #%2==0\n\u001b[1;32m    354\u001b[0m         if dict_config['save_model']:\n",
      "File \u001b[0;32m~/BIFOLD_work/domain_generalisation/conv_eeg/scripts/scripts.py:285\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(epoch, model, test_loader, bn, confusions, preload_reg)\u001b[0m\n\u001b[1;32m    283\u001b[0m else : \n\u001b[1;32m    284\u001b[0m     (data, target) = batch_data\n\u001b[0;32m--> 285\u001b[0m \n\u001b[1;32m    286\u001b[0m data, target = data.to(device), target.to(device)\n\u001b[1;32m    287\u001b[0m \n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from scripts.scripts import *\n",
    "\n",
    "p          = [0.2,160,4,70,5]\n",
    "dict_config = { \n",
    "'model':'EEGSimpleConv',\n",
    "'params':p,\n",
    "'dataset':'BNCI',\n",
    "'runs':5,\n",
    "'n_epochs':50,\n",
    "'EA':True,\n",
    "'mixup':True,\n",
    "'BN':True,\n",
    "'EOG':False,\n",
    "'Z':'Z0',\n",
    "'path':data_path,\n",
    "'lmso':False,\n",
    "'session':True,\n",
    "'reg_subject':True,\n",
    "'use_wandb':True,\n",
    "'evaluation':'cross',\n",
    "'comment':'baseline',\n",
    "'within':False,\n",
    "'mdl':False,\n",
    "'filter':0.5,\n",
    "'save_model':True,\n",
    "'save_model_path':'/home/nelosegui/BIFOLD_work/domain_generalisation/conv_eeg/results/BNCI_subj_reg',\n",
    "'load_model':False,\n",
    "'preload_reg':False,\n",
    "'online':False\n",
    "}    \n",
    "    \n",
    "    \n",
    "X,Y = load_data(dict_config)\n",
    "best_params = dict_config['params']\n",
    "best_score = train_test(best_params,dict_config, X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fae8f",
   "metadata": {},
   "source": [
    "Test for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb5738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Freq: 250\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X,Y \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BIFOLD_work/domain_generalisation/conv_eeg/scripts/scripts.py:64\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(dict_config)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Check \u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dict_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_subject\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dict_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m  :\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dict_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m7\u001b[39m   \n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X,Y = load_data(dict_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94553c-fe43-4f20-a810-f9c3b410b8f9",
   "metadata": {},
   "source": [
    "# Within-Subject on BNCI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767077b-1e57-4ce7-bf09-43e2781ff7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.scripts import *\n",
    "\n",
    "\n",
    "p          = [85,1,95,15]\n",
    "dict_config = { \n",
    "'model':'EEGSimpleConv',\n",
    "'params':p,\n",
    "'dataset':'BNCI',\n",
    "'runs':5,\n",
    "'n_epochs':50,\n",
    "'EA':True,\n",
    "'mixup':True,\n",
    "'BN':True,\n",
    "'EOG':False,\n",
    "'Z':'Z0',\n",
    "'path':'/users/local/simpleconv_datasets',\n",
    "'lmso':False,\n",
    "'session':True,\n",
    "'reg_subject':False,\n",
    "'use_wandb':False,\n",
    "'evaluation':'within',\n",
    "'comment':'baseline',\n",
    "'within':True,\n",
    "'mdl':False,\n",
    "'filter':0.5,\n",
    "'save_model':True,\n",
    "'save_model_path':'/nasbrain/y17eloua/models/BNCI_2',\n",
    "'load_model':False,\n",
    "'preload_reg':False,\n",
    "'online':False\n",
    "}  \n",
    "    \n",
    "    \n",
    "X,Y = load_data(dict_config)\n",
    "best_params = dict_config['params']\n",
    "best_score = train_test(best_params,dict_config, X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba8bee-9e4f-433a-8844-046e2e2192dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.scripts import *\n",
    "\n",
    "\n",
    "p          = [85,1,95,15]\n",
    "dict_config = { \n",
    "'model':'EEGSimpleConv',\n",
    "'params':p,\n",
    "'dataset':'BNCI',\n",
    "'runs':5,\n",
    "'n_epochs':50,\n",
    "'EA':True,\n",
    "'mixup':True,\n",
    "'BN':True,\n",
    "'EOG':False,\n",
    "'Z':'Z0',\n",
    "'path':'/users/local/simpleconv_datasets',\n",
    "'lmso':False,\n",
    "'session':True,\n",
    "'reg_subject':False,\n",
    "'use_wandb':False,\n",
    "'evaluation':'within',\n",
    "'comment':'baseline',\n",
    "'within':True,\n",
    "'mdl':False,\n",
    "'filter':0.5,\n",
    "'save_model':False,\n",
    "\n",
    "'load_model':False,\n",
    "'preload_reg':False,\n",
    "'online':False\n",
    "}  \n",
    "    \n",
    "    \n",
    "X,Y = load_data(dict_config)\n",
    "best_params = dict_config['params']\n",
    "best_score = train_test(best_params,dict_config, X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a8823-cb25-4aaa-b314-4a32cc86eb49",
   "metadata": {},
   "source": [
    "# Cross-Subject with Fine-Tuning on BNCI\n",
    "\n",
    "To run the following cell run the second code cell (Cross-Subject on BNCI with subject-wise reguralization first) with the argument save_model = True in its dict_config. It will provide the required pretrained model for each subject and each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22653f4-f3e7-4ef0-a5cc-abc21c8ab0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.scripts import *\n",
    "\n",
    "\n",
    "p          = [160,4,70,5]\n",
    "dict_config = { \n",
    "'model':'EEGSimpleConv',\n",
    "'params':p,\n",
    "'dataset':'BNCI',\n",
    "'runs':5,\n",
    "'n_epochs':60,\n",
    "'EA':True,\n",
    "'mixup':True,\n",
    "'BN':True,\n",
    "'EOG':False,\n",
    "'Z':'Z0',\n",
    "'path':'/users/local/simpleconv_datasets',\n",
    "'lmso':False,\n",
    "'session':True,\n",
    "'reg_subject':False,\n",
    "'use_wandb':False,\n",
    "'evaluation':'cross_finetune',\n",
    "'comment':'baseline',\n",
    "'within':True,\n",
    "'mdl':False,\n",
    "'filter':0.5,\n",
    "'save_model':True,\n",
    "'save_model_path':'/nasbrain/y17eloua/models/BNCI_2',   \n",
    "'load_model':True,\n",
    "'load_model_path':'/nasbrain/y17eloua/models/BNCI_2',\n",
    "'preload_reg':True,\n",
    "'online':False\n",
    "}\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "X,Y = load_data(dict_config)\n",
    "best_params = dict_config['params']\n",
    "best_score = train_test(best_params,dict_config, X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7209f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
